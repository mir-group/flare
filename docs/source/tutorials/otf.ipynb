{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-the-fly aluminum potential\n",
    "\n",
    "For the scheme/workflow of the on-the-fly training, please refer to [Vandermause et al.](https://www.nature.com/articles/s41524-020-0283-z). \n",
    "\n",
    "In our FLARE python package, we provide two ways for the on-the-fly training:\n",
    "1. Use `OTF` module and our MD engine (only supports NVE) $\\to$ this tutorial\n",
    "2. Use ASE `Atoms` with our ASE interface with `OTF` $\\to$ see [ase-tutorial](https://flare.readthedocs.io/en/latest/tutorials/ase.html)\n",
    "\n",
    "Here we give an example of running OTF (on-the-fly) training with QE (Quantum Espresso) and NVE ensemble.\n",
    "\n",
    "## Step 1: Set up a GP Model\n",
    "\n",
    "Let’s start up with the GP model with three-body kernel function. (See kernels.py (single component) or mc_simple.py (multi-component) for more options.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flare.gp import GaussianProcess\n",
    "\n",
    "# make gp model\n",
    "hyps = np.array([0.1, 1, 0.01])\n",
    "hyp_labels = ['Signal Std', 'Length Scale', 'Noise Std']\n",
    "cutoffs = {'threebody':3.9}\n",
    "\n",
    "gp = \\\n",
    "    GaussianProcess(kernels=['threebody'],\n",
    "                    hyps=hyps,\n",
    "                    cutoffs=cutoffs,\n",
    "                    hyp_labels=hyp_labels,\n",
    "                    maxiter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Explanation about the parameters**\n",
    "\n",
    "- `kernels`: set to be the name list of kernel functions to use\n",
    "    - Currently we have the choices of twobody, threebody and manybody kernel functions.\n",
    "    - If multiple kernels are listed, the resulted kernel is simply the summation of all listed kernels,\n",
    "\n",
    "- `hyps`: the array of hyperparameters, whose names are shown in `hyp_labels`.\n",
    "    - For two-body kernel function, an array of length 3 is needed, `hyps=[sigma_2, ls_2, sigma_n]`;\n",
    "    - For three-body, `hyps=[sigma_3, ls_3, sigma_n]`;\n",
    "    - For twobody plus threebody, `hyps=[sigma_2, ls_2, sigma_3, ls_3, sigma_n]`.\n",
    "    - For twobody, threebody plus manybody, `hyps=[sigma_2, ls_2, sigma_3, ls_3, sigma_m, ls_m, sigma_n]`.\n",
    "    \n",
    "- `cutoffs`: a dictionary consists of corresponding cutoff values for each kernel. Usually we will set a larger one for two-body, and smaller one for threebody and manybody\n",
    "\n",
    "- `maxiter`: set to constrain the number of steps in training hyperparameters.\n",
    "\n",
    "\n",
    "**Note**\n",
    "\n",
    "- See [GaussianProcess](https://flare.readthedocs.io/en/latest/flare/gp.html) for complete description of arguments of GaussianProcess class.\n",
    "- See [AdvancedHyperparametersSetUp](https://flare.readthedocs.io/en/latest/flare/utils/mask_helper.html) for more complicated hyper-parameters set up.\n",
    "\n",
    "## Step 2: Set up DFT Calculator\n",
    "\n",
    "The next step is to set up DFT calculator, here we use QE (quantum espresso). Suppose we’ve prepared a QE input file in current directory `./pwscf.in`, and have set the environment variable `PWSCF_COMMAND` to the location of our QE’s executable `pw.x`. Then we specify the input file and executable by `qe_input` and `dft_loc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set up DFT calculator\n",
    "qe_input = './pwscf.in' # quantum espresso input file\n",
    "dft_loc = os.environ.get('PWSCF_COMMAND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up OTF MD Training Engine\n",
    "\n",
    "Then we can set up our On-The-Fly (OTF) MD engine for training and simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up OTF parameters\n",
    "dt = 0.001                  # timestep (ps)\n",
    "number_of_steps = 100       # number of steps\n",
    "std_tolerance_factor = 1\n",
    "max_atoms_added = 2\n",
    "freeze_hyps = 3\n",
    "\n",
    "otf = OTF(qe_input, dt, number_of_steps, gp, dft_loc,\n",
    "          std_tolerance_factor, init_atoms=[0],\n",
    "          calculate_energy=True, output_name='al_otf_qe',\n",
    "          freeze_hyps=freeze_hyps, skip=5,\n",
    "          max_atoms_added=max_atoms_added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Explanation about the parameters**\n",
    "\n",
    "- `dt`: the time step in unit of ps\n",
    "\n",
    "- `number_of_steps`: the number of steps that the MD is run\n",
    "\n",
    "- `std_tolerance_factor`: the *uncertainty threshold = std_tolerance_factor x hyps[-1]*. In OTF training, when GP predicts uncertainty above the uncertainty threshold, it will call DFT\n",
    "\n",
    "- `max_atoms_added`: constrain the number of atoms added to the training set after each DFT call\n",
    "\n",
    "- `freeze_hyps`: stop training hyperparameters and fix them from the `freeze_hyps` th step. Usually set to a small number, because for large dataset the training will take long.\n",
    "\n",
    "- `init_atoms`: list of atoms to be added in the first DFT call. Because there’s no uncertainty predicted in the initial DFT call, so there’s no selection rule to pick up “maximully uncertain” atoms into the training set, we have to specify which atoms to pick up by this variable.\n",
    "\n",
    "- `calculate_energy`: if True, the local energy on each atom will be calculated\n",
    "\n",
    "- `output_name`: the name of the logfile\n",
    "\n",
    "- `skip`: record/dump the information every skip steps.\n",
    "\n",
    "## Step 4: Launch the OTF Training\n",
    "Finally, let’s run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run OTF MD\n",
    "otf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After OTF training is finished, we can check log file `al_otf_qe.out` for all the information dumped. This output file can be parsed using our `otf_parser.py` module, which will be introduced in the [after-training-tutorial](https://flare.readthedocs.io/en/latest/tutorials/after_training.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
